{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_groq in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.9)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_groq) (0.9.0)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_groq) (0.2.33)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (0.1.99)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\parth\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (8.2.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\parth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE THE LLM\n",
    "\n",
    "ADD THE HIDDEN API KEY AND THE MODEL NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "llm = ChatGroq(model=groq_models[model_name] , api_key=groq_api_key, temperature=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAKING 6 TRAIN & TEST INDIVIVALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train_sub1_laying=pd.read_csv(r\"../Dataset/Combined/Train/LAYING/Subject_1.csv\")\n",
    "Train_sub3_sitting=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\SITTING\\Subject_3.csv\")\n",
    "Train_sub5_standing=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\Standing\\Subject_5.csv\")\n",
    "Train_sub7_walking=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\LAYING\\Subject_7.csv\")\n",
    "Train_sub8_walking_downstairs=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_8.csv\")\n",
    "Train_sub11_walking_upstairs=pd.read_csv(r\"..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_11.csv\")\n",
    "\n",
    "\n",
    "Test_sub2_laying=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\LAYING\\Subject_2.csv\")\n",
    "Test_sub9_sitting=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\SITTING\\Subject_9.csv\")\n",
    "Test_sub10_standing=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\Standing\\Subject_10.csv\")\n",
    "Test_sub4_walking=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\LAYING\\Subject_4.csv\")\n",
    "Test_sub20_walking_downstairs=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_20.csv\")\n",
    "Test_sub24_walking_upstairs=pd.read_csv(r\"..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_24.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAKING THE AVG OF ALL THE ACC IN X,Y,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean calculations complete. Test and train mean datasets have been combined and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_path = r\"..\\Dataset\\Combined\"\n",
    "\n",
    "# Define test subjects\n",
    "test_subjects = {\n",
    "    'LAYING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'SITTING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'Standing': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING_DOWNSTAIRS': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING_UPSTAIRS': [2, 4, 9, 10, 12, 13, 18, 20, 24]\n",
    "}\n",
    "\n",
    "# Define train subjects (excluding test subjects)\n",
    "train_subjects = {\n",
    "    'LAYING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'SITTING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'Standing': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING_DOWNSTAIRS': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING_UPSTAIRS': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30]\n",
    "}\n",
    "\n",
    "# Function to calculate mean of acceleration values\n",
    "def load_and_mean(activity, subjects, data_type='Train'):\n",
    "    means = []\n",
    "    for subject in subjects:\n",
    "        file_path = os.path.join(base_path, data_type, activity, f'Subject_{subject}.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            mean_values = df[['accx', 'accy', 'accz']].mean()\n",
    "            mean_values['Activity'] = activity\n",
    "            mean_values['Subject'] = subject\n",
    "            means.append(mean_values)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    return pd.DataFrame(means)\n",
    "\n",
    "# Load and process test data\n",
    "test_means = {activity: load_and_mean(activity, subjects, 'Test') for activity, subjects in test_subjects.items()}\n",
    "\n",
    "# Load and process train data\n",
    "train_means = {activity: load_and_mean(activity, [sub for sub in train_subjects[activity] if sub not in test_subjects[activity]], 'Train') for activity in train_subjects}\n",
    "\n",
    "# Combine all activities for test and train mean data\n",
    "test_mean_df = pd.concat(test_means.values(), ignore_index=True)\n",
    "train_mean_df = pd.concat(train_means.values(), ignore_index=True)\n",
    "\n",
    "# Save combined mean DataFrames to CSV if needed\n",
    "test_mean_df.to_csv('test_data_means.csv', index=False)\n",
    "train_mean_df.to_csv('train_data_means.csv', index=False)\n",
    "\n",
    "print(\"Mean calculations complete. Test and train mean datasets have been combined and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGING THE WHOLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\LAYING\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\SITTING\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\Standing\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_DOWNSTAIRS\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_2.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_4.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_9.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_10.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_12.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_13.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_18.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_20.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Test\\WALKING_UPSTAIRS\\Subject_24.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\LAYING\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\SITTING\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\Standing\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_DOWNSTAIRS\\Subject_30.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_1.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_3.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_5.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_6.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_7.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_8.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_11.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_14.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_15.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_16.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_17.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_19.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_21.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_22.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_23.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_25.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_26.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_27.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_28.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_29.csv\n",
      "Checking file: ..\\Dataset\\Combined\\Train\\WALKING_UPSTAIRS\\Subject_30.csv\n",
      "Data merging complete. Test and train datasets (first 500 rows) have been combined and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_path = r\"..\\Dataset\\Combined\"\n",
    "\n",
    "# Define test subjects\n",
    "test_subjects = {\n",
    "    'LAYING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'SITTING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'Standing': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING_DOWNSTAIRS': [2, 4, 9, 10, 12, 13, 18, 20, 24],\n",
    "    'WALKING_UPSTAIRS': [2, 4, 9, 10, 12, 13, 18, 20, 24]\n",
    "}\n",
    "\n",
    "# Define train subjects (excluding test subjects)\n",
    "train_subjects = {\n",
    "    'LAYING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'SITTING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'Standing': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING_DOWNSTAIRS': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30],\n",
    "    'WALKING_UPSTAIRS': [1, 3, 5, 6, 7, 8, 11, 14, 15, 16, 17, 19, 21, 22 , 23, 25,26,27,28,29,30]\n",
    "}\n",
    "\n",
    "# Function to load the first 500 rows of each dataset\n",
    "def load_data(activity, subjects, data_type='Train'):\n",
    "    data_list = []\n",
    "    for subject in subjects:\n",
    "        file_path = os.path.join(base_path, data_type, activity, f'Subject_{subject}.csv')\n",
    "        print(f\"Checking file: {file_path}\")  # Debugging line\n",
    "        if os.path.exists(file_path):\n",
    "            # Read the first 500 rows\n",
    "            df = pd.read_csv(file_path, nrows=500)\n",
    "            df['Activity'] = activity\n",
    "            df['Subject'] = subject\n",
    "            data_list.append(df)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    return pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Load and process test data\n",
    "test_data = {activity: load_data(activity, subjects, 'Test') for activity, subjects in test_subjects.items()}\n",
    "\n",
    "# Load and process train data\n",
    "train_data = {activity: load_data(activity, [sub for sub in train_subjects[activity] if sub not in test_subjects[activity]], 'Train') for activity in train_subjects}\n",
    "\n",
    "# Combine all activities for test and train data\n",
    "test_data_df = pd.concat(test_data.values(), ignore_index=True)\n",
    "train_data_df = pd.concat(train_data.values(), ignore_index=True)\n",
    "\n",
    "# Save combined DataFrames to CSV if needed\n",
    "test_data_df.to_csv('test_data_combined.csv', index=False)\n",
    "train_data_df.to_csv('train_data_combined.csv', index=False)\n",
    "\n",
    "print(\"Data merging complete. Test and train datasets (first 500 rows) have been combined and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ZERO-SHOT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE I\n",
    "\n",
    "only 6 test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Classification: Based on the provided accelerometer data, I will predict the corresponding human activity labels. Here are the predictions:\n",
      "\n",
      "**Case 1:**\n",
      "accx = -0.023458, accy = 0.768965, accz = 0.630654\n",
      "**Predicted Activity:** STANDING\n",
      "\n",
      "**Case 2:**\n",
      "accx = 0.974312, accy = -0.031424, accz = 0.095099\n",
      "**Predicted Activity:** WALKING\n",
      "\n",
      "**Case 3:**\n",
      "accx = 1.010296, accy = -0.024385, accz = 0.028739\n",
      "**Predicted Activity:** WALKING\n",
      "\n",
      "**Case 4:**\n",
      "accx = 0.035202, accy = 0.933609, accz = 0.328544\n",
      "**Predicted Activity:** STANDING\n",
      "\n",
      "**Case 5:**\n",
      "accx = 0.987812, accy = -0.253559, accz = -0.030405\n",
      "**Predicted Activity:** WALKING DOWNWARDS\n",
      "\n",
      "**Case 6:**\n",
      "accx = 0.973563, accy = -0.222988, accz = -0.231508\n",
      "**Predicted Activity:** WALKING DOWNWARDS\n",
      "\n",
      "Please note that these predictions are based on the patterns and relationships learned from the accelerometer data and may not be 100% accurate.\n"
     ]
    }
   ],
   "source": [
    "query_zero_shot_1 = f\"\"\"\n",
    "* You are an activity classification model.\n",
    "* Your task is to predict the human activity based on the given accelerometer data.\n",
    "* The accelerometer data contains acceration in x , y , z respectfully.\n",
    "* Provide the predicted activity label based on the data.\n",
    "* You are given data of 6 different case.\n",
    "* classify them into LAYYING,STANDIN,WALKING,SITTING,WALKING UPWARDS,WALKING DOWNWARDS\n",
    "\n",
    "Accelerometer Data: {Test_sub2_laying.mean()},{Test_sub9_sitting.mean()},{Test_sub10_standing.mean()},{Test_sub4_walking.mean()},{Test_sub20_walking_downstairs.mean()}\n",
    ",{Test_sub24_walking_upstairs.mean()}\"\"\"\n",
    "\n",
    "response_zero_shot_1 = llm.invoke(query_zero_shot_1)\n",
    "print(\"Zero-Shot Classification:\", response_zero_shot_1.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33.33 percent are or 2/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE II\n",
    "\n",
    "Zero shot for whole data with the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Classification: Based on the provided accelerometer data, I'll classify the activities from 0 to 53. Here are the results:\n",
      "\n",
      "0. WALKING\n",
      "1. WALKING\n",
      "2. WALKING\n",
      "3. WALKING\n",
      "4. WALKING\n",
      "5. WALKING\n",
      "6. WALKING\n",
      "7. WALKING\n",
      "8. WALKING\n",
      "9. WALKING\n",
      "10. WALKING\n",
      "11. WALKING\n",
      "12. WALKING\n",
      "13. WALKING\n",
      "14. WALKING\n",
      "15. WALKING\n",
      "16. WALKING\n",
      "17. WALKING\n",
      "18. WALKING\n",
      "19. WALKING\n",
      "20. WALKING\n",
      "21. WALKING\n",
      "22. WALKING\n",
      "23. WALKING\n",
      "24. WALKING\n",
      "25. WALKING\n",
      "26. WALKING\n",
      "27. WALKING\n",
      "28. WALKING\n",
      "29. WALKING\n",
      "30. WALKING\n",
      "31. WALKING\n",
      "32. WALKING\n",
      "33. WALKING\n",
      "34. WALKING\n",
      "35. WALKING\n",
      "36. WALKING\n",
      "37. WALKING\n",
      "38. WALKING\n",
      "39. WALKING\n",
      "40. WALKING\n",
      "41. WALKING\n",
      "42. WALKING\n",
      "43. WALKING\n",
      "44. WALKING\n",
      "45. WALKING DOWNSTAIRS\n",
      "46. WALKING\n",
      "47. WALKING\n",
      "48. WALKING\n",
      "49. WALKING\n",
      "50. WALKING DOWNSTAIRS\n",
      "51. WALKING DOWNSTAIRS\n",
      "52. WALKING DOWNSTAIRS\n",
      "53. WALKING DOWNSTAIRS\n"
     ]
    }
   ],
   "source": [
    "query_zero_shot_2 = f\"\"\"\n",
    "* You are a highly accurate activity classification model.\n",
    "* Your task is to classify human activities based on the given accelerometer data.\n",
    "* The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "* You are given data corresponding to six different activities.\n",
    "* The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\n",
    "* Analyze the accelerometer data and provide the most likely activity label for each case.\n",
    "\n",
    "accx , accy, accz = {test_mean_df[\"accx\"]},{test_mean_df[\"accy\"]},{test_mean_df[\"accz\"]}\n",
    "\n",
    "just print the result of all from zero to 53 th\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example code to invoke the model\n",
    "response_zero_shot_2 = llm.invoke(query_zero_shot_2)\n",
    "print(\"Zero-Shot Classification:\", response_zero_shot_2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mean_df['Prediction']  =   [\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING DOWNSTAIRS\", \"WALKING\", \"WALKING\", \"WALKING\", \"WALKING\",\n",
    "    \"WALKING DOWNSTAIRS\", \"WALKING DOWNSTAIRS\", \"WALKING DOWNSTAIRS\", \"WALKING DOWNSTAIRS\"\n",
    "]\n",
    "len(test_mean_df)\n",
    "\n",
    "new_accuracy = (test_mean_df['Activity'] == test_mean_df['Prediction']).mean()\n",
    "new_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this very low is because we pass the mean of all the data and we lost too much data and the acc in x,y,z became constant while it should not be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE III\n",
    "\n",
    "Zero shot for whole data with the range of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "1\n",
      "WALKING WALKING\n",
      "2\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "3\n",
      "WALKING WALKING\n",
      "4\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "5\n",
      "SITTING WALKING\n",
      "Standing WALKING\n",
      "WALKING WALKING\n",
      "6\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "7\n",
      "WALKING WALKING\n",
      "8\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "9\n",
      "WALKING WALKING\n",
      "10\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "11\n",
      "WALKING WALKING\n",
      "12\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "13\n",
      "WALKING WALKING\n",
      "14\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "15\n",
      "WALKING WALKING\n",
      "16\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING STANDING\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "17\n",
      "WALKING WALKING\n",
      "18\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "33.33333333333333\n",
      "Classification complete. Results have been saved to 'model_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming you have the test data loaded as a DataFrame\n",
    "test_data_df = pd.read_csv('test_data_combined.csv')\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING UPSTAIRS', 'WALKING DOWNSTAIRS']\n",
    "\n",
    "# Function to sample data\n",
    "def sample_data(data, sample_rate=15):\n",
    "    n=500//sample_rate\n",
    "    return data[:n]\n",
    "i=0\n",
    "# Iterate through each group by subject and activity\n",
    "for (subject, activity), group in test_data_df.groupby(['Subject', 'Activity']):\n",
    "    # Sample data\n",
    "    accx = sample_data(group['accx'].tolist())\n",
    "    accy = sample_data(group['accy'].tolist())\n",
    "    accz = sample_data(group['accz'].tolist())\n",
    "    \n",
    "    # Create prompt for the current group\n",
    "    query_zero_shot_3 = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given accelerometer data.\n",
    "    * The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "    * You are given data corresponding to six different activities.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\n",
    "    * Analyze the accelerometer data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTAIN NO REASON JUST A PREDICTION\n",
    "\n",
    "    accx = {accx}\n",
    "    accy = {accy}\n",
    "    accz = {accz}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_zero_shot_3)        \n",
    "        \n",
    "    print(activity,str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if (activity.upper() ==str(result).split(\" \")[0][8:].strip(\"'\")):\n",
    "        i+=1\n",
    "        print(i)\n",
    "\n",
    "    results.append(str(result).split(\" \")[0][8:])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "results_df.to_csv('model_predictions_zeroshot.csv', index=False)\n",
    "\n",
    "print(i/54*100)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33.33 PERCENT ACUURACY FOR ZERO-SHOT OVER ALL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE IV\n",
    "\n",
    "zero shot with PCA of TSFEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SITTING STANDING\n",
      "SITTING STANDING\n",
      "LAYING LAYING\n",
      "1\n",
      "STANDING STANDING\n",
      "2\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "LAYING LAYING\n",
      "3\n",
      "WALKING_UPSTAIRS STANDING\n",
      "SITTING STANDING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "WALKING STANDING\n",
      "SITTING STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING STANDING\n",
      "STANDING STANDING\n",
      "4\n",
      "WALKING STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "SITTING STANDING\n",
      "WALKING STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "STANDING STANDING\n",
      "5\n",
      "WALKING_UPSTAIRS STANDING\n",
      "SITTING STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "Classification complete. Results have been saved to 'model_predictions_zeroshot.csv'.\n",
      "accuracy 19.230769230769234\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the PCA data\n",
    "df_pca = pd.read_csv('df_pca.csv')\n",
    "\n",
    "# Split the data into train and test sets ensuring no overlap\n",
    "train_data_df, test_data_df = train_test_split(df_pca, test_size=0.2, random_state=42)\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS']\n",
    "\n",
    "def sample_data(data, sample_rate=10):\n",
    "    n = 500 // sample_rate\n",
    "    return data[:n]\n",
    "\n",
    "i = 0\n",
    "# Iterate through each row in the test set\n",
    "for idx, row in test_data_df.iterrows():\n",
    "    # Sample data for the current row\n",
    "    pc1 = sample_data([row['PC1']])\n",
    "    pc2 = sample_data([row['PC2']])\n",
    "    activity = row['Activity']\n",
    "\n",
    "    query_zero_shot = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given PCA data.\n",
    "    * The PCA data is derived from TSFEL features extended from accelerometer data in the x, y, and z directions (accx, accy, accz).\n",
    "    * The PCA data is provided as values in the PC1 and PC2 directions.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING_UPSTAIRS, and WALKING_DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    query_zero_shot += f\"\"\"\n",
    "    * Analyze the PCA data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    PC1 = {pc1}\n",
    "    PC2 = {pc2}\n",
    "    \"\"\"\n",
    "    \n",
    "    result = llm.invoke(query_zero_shot)\n",
    "    \n",
    "    print(activity, str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if activity.upper() == str(result).split(\" \")[0][8:].strip(\"'\"):\n",
    "        i += 1\n",
    "        print(i)\n",
    "    results.append({'Activity': activity, 'Prediction': str(result).split(\" \")[0][8:].strip(\"'\")})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_csv('model_predictions_zeroshot.csv', index=False)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions_zeroshot.csv'.\")\n",
    "\n",
    "print(\"accuracy\", i/len(test_data_df)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# FEW SHOT\n",
    "> ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE I\n",
    "\n",
    "only 6 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SITTING STANDING\n",
      "SITTING STANDING\n",
      "LAYING LAYING\n",
      "1\n",
      "STANDING STANDING\n",
      "2\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "LAYING LAYING\n",
      "3\n",
      "WALKING_UPSTAIRS STANDING\n",
      "SITTING STANDING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "WALKING STANDING\n",
      "SITTING STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING STANDING\n",
      "STANDING STANDING\n",
      "4\n",
      "WALKING STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "SITTING STANDING\n",
      "WALKING STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "STANDING STANDING\n",
      "5\n",
      "WALKING_UPSTAIRS STANDING\n",
      "SITTING STANDING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "Classification complete. Results have been saved to 'model_predictions_zeroshot.csv'.\n",
      "accuracy 19.230769230769234\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the PCA data\n",
    "df_pca = pd.read_csv('df_pca.csv')\n",
    "\n",
    "# Split the data into train and test sets ensuring no overlap\n",
    "train_data_df, test_data_df = train_test_split(df_pca, test_size=0.2, random_state=42)\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS']\n",
    "\n",
    "def sample_data(data, sample_rate=10):\n",
    "    n = 500 // sample_rate\n",
    "    return data[:n]\n",
    "\n",
    "i = 0\n",
    "# Iterate through each row in the test set\n",
    "for idx, row in test_data_df.iterrows():\n",
    "    # Sample data for the current row\n",
    "    pc1 = sample_data([row['PC1']])\n",
    "    pc2 = sample_data([row['PC2']])\n",
    "    activity = row['Activity']\n",
    "\n",
    "    # Create a zero-shot prompt\n",
    "    query_zero_shot = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given PCA data.\n",
    "    * The PCA data is derived from TSFEL features extended from accelerometer data in the x, y, and z directions (accx, accy, accz).\n",
    "    * The PCA data is provided as values in the PC1 and PC2 directions.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING_UPSTAIRS, and WALKING_DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    # Add the test example\n",
    "    query_zero_shot += f\"\"\"\n",
    "    * Analyze the PCA data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    PC1 = {pc1}\n",
    "    PC2 = {pc2}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_zero_shot)\n",
    "    \n",
    "    print(activity, str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if activity.upper() == str(result).split(\" \")[0][8:].strip(\"'\"):\n",
    "        i += 1\n",
    "        print(i)\n",
    "    results.append({'Activity': activity, 'Prediction': str(result).split(\" \")[0][8:].strip(\"'\")})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "results_df.to_csv('model_predictions_zeroshot.csv', index=False)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions_zeroshot.csv'.\")\n",
    "\n",
    "print(\"accuracy\", i/len(test_data_df)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE II\n",
    "\n",
    "Few shot for whole data with the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Classification: Based on the provided accelerometer data, I will classify the activities as follows:\n",
      "\n",
      "0. LAYING\n",
      "1. LAYING\n",
      "2. LAYING\n",
      "3. LAYING\n",
      "4. LAYING\n",
      "5. LAYING\n",
      "6. LAYING\n",
      "7. LAYING\n",
      "8. LAYING\n",
      "9. WALKING_UPSTAIRS\n",
      "10. WALKING_UPSTAIRS\n",
      "11. WALKING_UPSTAIRS\n",
      "12. WALKING_UPSTAIRS\n",
      "13. WALKING_UPSTAIRS\n",
      "14. WALKING_UPSTAIRS\n",
      "15. WALKING_UPSTAIRS\n",
      "16. WALKING_UPSTAIRS\n",
      "17. WALKING_UPSTAIRS\n",
      "18. WALKING_UPSTAIRS\n",
      "19. WALKING_UPSTAIRS\n",
      "20. WALKING_UPSTAIRS\n",
      "21. WALKING_UPSTAIRS\n",
      "22. WALKING_UPSTAIRS\n",
      "23. WALKING_UPSTAIRS\n",
      "24. WALKING_UPSTAIRS\n",
      "25. WALKING_UPSTAIRS\n",
      "26. WALKING_UPSTAIRS\n",
      "27. WALKING_UPSTAIRS\n",
      "28. WALKING_UPSTAIRS\n",
      "29. WALKING_UPSTAIRS\n",
      "30. WALKING_UPSTAIRS\n",
      "31. WALKING_UPSTAIRS\n",
      "32. WALKING_UPSTAIRS\n",
      "33. WALKING_UPSTAIRS\n",
      "34. WALKING_UPSTAIRS\n",
      "35. WALKING_UPSTAIRS\n",
      "36. WALKING_UPSTAIRS\n",
      "37. WALKING_UPSTAIRS\n",
      "38. WALKING_UPSTAIRS\n",
      "39. WALKING_UPSTAIRS\n",
      "40. WALKING_UPSTAIRS\n",
      "41. WALKING_UPSTAIRS\n",
      "42. WALKING_UPSTAIRS\n",
      "43. WALKING_UPSTAIRS\n",
      "44. WALKING_UPSTAIRS\n",
      "45. WALKING_DOWNSTAIRS\n",
      "46. WALKING_DOWNSTAIRS\n",
      "47. WALKING_DOWNSTAIRS\n",
      "48. WALKING_DOWNSTAIRS\n",
      "49. WALKING_DOWNSTAIRS\n",
      "50. WALKING_DOWNSTAIRS\n",
      "51. WALKING_DOWNSTAIRS\n",
      "52. WALKING_DOWNSTAIRS\n",
      "53. WALKING_DOWNSTAIRS\n",
      "\n",
      "Note that the classification is based on the patterns and characteristics of the accelerometer data, and the results may not be 100% accurate.\n"
     ]
    }
   ],
   "source": [
    "query_few_shot = f\"\"\"\n",
    "* You are an activity classification model.\n",
    "* Your task is to classify human activities based on the given accelerometer data.\n",
    "* The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "* The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\n",
    "\n",
    "Here are some labeled examples:\n",
    "\n",
    "{train_mean_df}\n",
    "\n",
    "Now, based on the provided examples, classify the following new accelerometer data:\n",
    "predict it with accx , accy, accz = {test_mean_df[\"accx\"]},{test_mean_df[\"accy\"]},{test_mean_df[\"accz\"]}\n",
    "\n",
    "just print the result of all from zero to last\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Example code to invoke the model\n",
    "response_few_shot_2 = llm.invoke(query_few_shot)\n",
    "print(\"Few-Shot Classification:\", response_few_shot_2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mean_df['Prediction']  =  [\n",
    "    \"LAYING\", \"LAYING\", \"LAYING\", \"LAYING\", \"LAYING\",\n",
    "    \"LAYING\", \"LAYING\", \"LAYING\", \"LAYING\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\", \"WALKING_UPSTAIRS\",  \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\", \"WALKING_DOWNSTAIRS\"\n",
    "]\n",
    "\n",
    "\n",
    "new_accuracy = (test_mean_df['Activity'] == test_mean_df['Prediction']).mean()\n",
    "new_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this very low is because we pass the mean of all the data and we lost too much data and the acc in x,y,z became constant while it should not be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE III\n",
    "\n",
    " Few shot for whole data with the range of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYING LAYING\n",
      "1\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "2\n",
      "WALKING WALKING\n",
      "3\n",
      "WALKING_DOWNSTAIRS WALKING_UPSTAIRS\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "4\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "5\n",
      "WALKING WALKING\n",
      "6\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "7\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "8\n",
      "WALKING WALKING\n",
      "9\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "10\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "11\n",
      "WALKING WALKING\n",
      "12\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "13\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "14\n",
      "WALKING WALKING\n",
      "15\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "16\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "17\n",
      "WALKING WALKING\n",
      "18\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "19\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "20\n",
      "WALKING WALKING\n",
      "21\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "22\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "23\n",
      "WALKING WALKING\n",
      "24\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "LAYING LAYING\n",
      "25\n",
      "SITTING STANDING\n",
      "Standing STANDING\n",
      "26\n",
      "WALKING WALKING\n",
      "27\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "WALKING_UPSTAIRS WALKING\n",
      "Classification complete. Results have been saved to 'model_predictions.csv'.\n",
      "accuracy 50.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the test data loaded as a DataFrame\n",
    "test_data_df = pd.read_csv('test_data_combined.csv')\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING UPSTAIRS', 'WALKING DOWNSTAIRS']\n",
    "\n",
    "def sample_data(data, sample_rate=10):\n",
    "    n=500//sample_rate\n",
    "    return data[:n]\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Filter training data for specific subjects\n",
    "train_subjects = [1, 3, 5, 7]\n",
    "train_data_df = pd.read_csv('train_data_combined.csv')\n",
    "train_data_df = train_data_df[train_data_df['Subject'].isin(train_subjects)]\n",
    "\n",
    "# Iterate through each group by subject and activity\n",
    "for (subject, activity), group in test_data_df.groupby(['Subject', 'Activity']):\n",
    "    # Sample data\n",
    "    accx = sample_data(group['accx'].tolist())\n",
    "    accy = sample_data(group['accy'].tolist())\n",
    "    accz = sample_data(group['accz'].tolist())\n",
    "    \n",
    "    # Create prompt for the current group\n",
    "    query_few_shot_3 = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given accelerometer data.\n",
    "    * The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "    * You are given data corresponding to six different activities.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    for (train_subject, train_activity), grp in train_data_df.groupby(['Subject', 'Activity']):\n",
    "        query_few_shot_3 += \"\"\"Here are some examples of accelerometer data and their corresponding activities:\"\"\"\n",
    "\n",
    "        query_few_shot_3 += f\"\"\"\n",
    "        * Activity: {train_activity}\n",
    "          accx = {sample_data(grp['accx'].tolist(), 50)}\n",
    "          accy = {sample_data(grp['accy'].tolist(), 50)}\n",
    "          accz = {sample_data(grp['accz'].tolist(), 50)}\n",
    "        \"\"\"\n",
    "    \n",
    "    query_few_shot_3 += f\"\"\"\n",
    "    * Analyze the accelerometer data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    accx = {accx}\n",
    "    accy = {accy}\n",
    "    accz = {accz}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_few_shot_3)\n",
    "    \n",
    "    print(activity, str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if (activity.upper() ==str(result).split(\" \")[0][8:].strip(\"'\")):\n",
    "        i+=1\n",
    "        print(i)\n",
    "    results.append({'Subject': subject, 'Activity': activity, 'Prediction': str(result).split(\" \")[0][8:].strip(\"'\")})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "results_df.to_csv('model_predictions_fewshot.csv', index=False)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions.csv'.\")\n",
    "\n",
    "print(\"accuracy\",i/54*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE IV\n",
    "\n",
    "Few shot with PCA of TSFEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SITTING STANDING\n",
      "SITTING SITTING\n",
      "1\n",
      "LAYING LAYING\n",
      "2\n",
      "STANDING STANDING\n",
      "3\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING_UPSTAIRS WALKING_UPSTAIRS\n",
      "4\n",
      "WALKING_UPSTAIRS STANDING\n",
      "LAYING LAYING\n",
      "5\n",
      "WALKING_UPSTAIRS WALKING\n",
      "SITTING STANDING\n",
      "WALKING_UPSTAIRS STANDING\n",
      "WALKING STANDING\n",
      "SITTING STANDING\n",
      "WALKING_DOWNSTAIRS LAYING\n",
      "WALKING_DOWNSTAIRS STANDING\n",
      "WALKING WALKING_UPSTAIRS\n",
      "STANDING STANDING\n",
      "6\n",
      "WALKING STANDING\n",
      "WALKING_DOWNSTAIRS LAYING\n",
      "SITTING LAYING\n",
      "WALKING WALKING\n",
      "7\n",
      "WALKING_DOWNSTAIRS WALKING_UPSTAIRS\n",
      "STANDING STANDING\n",
      "8\n",
      "WALKING_UPSTAIRS STANDING\n",
      "SITTING STANDING\n",
      "WALKING_DOWNSTAIRS WALKING\n",
      "Classification complete. Results have been saved to 'model_predictions_fewshot.csv'.\n",
      "accuracy 30.76923076923077\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the PCA data\n",
    "df_pca = pd.read_csv('df_pca.csv')\n",
    "\n",
    "# Split the data into train and test sets ensuring no overlap\n",
    "train_data_df, test_data_df = train_test_split(df_pca, test_size=0.2, random_state=42)\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Define the activities\n",
    "activities = ['LAYING', 'STANDING', 'WALKING', 'SITTING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS']\n",
    "\n",
    "def sample_data(data, sample_rate=10):\n",
    "    n = 500 // sample_rate\n",
    "    return data[:n]\n",
    "\n",
    "i = 0\n",
    "# Iterate through each row in the test set\n",
    "for idx, row in test_data_df.iterrows():\n",
    "    # Sample data for the current row\n",
    "    pc1 = sample_data([row['PC1']])\n",
    "    pc2 = sample_data([row['PC2']])\n",
    "    activity = row['Activity']\n",
    "\n",
    "    # Create a few-shot prompt\n",
    "    query_few_shot = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given PCA data.\n",
    "    * The PCA data is derived from TSFEL features extended from accelerometer data in the x, y, and z directions (accx, accy, accz).\n",
    "    * The PCA data is provided as values in the PC1 and PC2 directions.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING_UPSTAIRS, and WALKING_DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    # Add training examples from the training set\n",
    "    for train_activity, grp in train_data_df.groupby('Activity'):\n",
    "        query_few_shot += \"\"\"Here are some examples of PCA data and their corresponding activities:\"\"\"\n",
    "        query_few_shot += f\"\"\"\n",
    "        * Activity: {train_activity}\n",
    "          PC1 = {sample_data(grp['PC1'].tolist(), 50)}\n",
    "          PC2 = {sample_data(grp['PC2'].tolist(), 50)}\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add the test example\n",
    "    query_few_shot += f\"\"\"\n",
    "    * Analyze the PCA data and provide the most likely activity label for each case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    PC1 = {pc1}\n",
    "    PC2 = {pc2}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_few_shot)\n",
    "    \n",
    "    print(activity, str(result).split(\" \")[0][8:].strip(\"'\"))\n",
    "    if activity.upper() == str(result).split(\" \")[0][8:].strip(\"'\"):\n",
    "        i += 1\n",
    "        print(i)\n",
    "    results.append({'Activity': activity, 'Prediction': str(result).split(\" \")[0][8:].strip(\"'\")})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV if needed\n",
    "results_df.to_csv('model_predictions_fewshot.csv', index=False)\n",
    "\n",
    "print(\"Classification complete. Results have been saved to 'model_predictions_fewshot.csv'.\")\n",
    "\n",
    "print(\"accuracy\", i/len(test_data_df)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30.77 PERCENT ACCURACY OF FEW-SHOT OVERALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ANSWER TO THE QUESTIONS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A1)\n",
    " Few shots are better it is give examples and has a idea of how the inputs will be for a particular activity while the zero-shot only has generic information nothing in particular.\n",
    "\n",
    ">A2)\n",
    " decision trees accuracy was  61.1% max(from task 2) while the accuracy of the few-shots is 46.3% so the decision is better because it sets up a threshold value and froms more structed value than that in few-shots. and we were also able to give much more data  in the decision tree we may recive closer accuracy it the number of token would have not been limited.\n",
    "\n",
    ">A3) \n",
    "few shot depends a lot on the exmaples given in it so it should be given a different varity of samples other wise it will be overfitted on the data given and will have high bais. and both zero shot and few shot struggles for new data which they have not seen before.\n",
    "\n",
    ">A4)\n",
    " few shot is more likely to predict it as the data which is seen unless there is significant difference between the new unseen sample and the train data set. In zero shot there is not train data so it will predict it to the closes activity possible based on generic information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A5)\n",
    " most of the result shown are dynamic as I did unifrom distribution over -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       accx      accy      accz\n",
      "0 -0.600466 -0.424734 -0.046532\n",
      "1 -0.538894 -0.496867 -0.358970\n",
      "2  0.478016 -0.793912 -0.274391\n",
      "3  0.636616 -0.083124  0.131175\n",
      "4  0.860025  0.155066  0.793801\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0 -0.291719  0.863479  0.004827\n",
      "1  0.129610 -0.401998 -0.090099\n",
      "2  0.038629 -0.431866 -0.924246\n",
      "3  0.204572  0.147227 -0.650012\n",
      "4 -0.135920 -0.535460 -0.043527\n",
      "Prediction for the random data: ='WALKING'\n",
      "       accx      accy      accz\n",
      "0 -0.018285 -0.323576  0.476706\n",
      "1 -0.967154  0.013055  0.510384\n",
      "2  0.952484  0.288089 -0.736565\n",
      "3 -0.729330 -0.305223  0.949006\n",
      "4  0.531035  0.232746 -0.632187\n",
      "Prediction for the random data: ='WALKING'\n",
      "       accx      accy      accz\n",
      "0  0.515219 -0.347614 -0.654486\n",
      "1  0.570632  0.110917  0.028251\n",
      "2  0.903048 -0.649447  0.419353\n",
      "3 -0.435997 -0.682641  0.521948\n",
      "4  0.436086 -0.986751  0.886106\n",
      "Prediction for the random data: ='WALKING'\n",
      "       accx      accy      accz\n",
      "0 -0.549071  0.883861  0.211522\n",
      "1  0.493467  0.469254 -0.869282\n",
      "2 -0.327979 -0.820570  0.060077\n",
      "3 -0.681408 -0.100256  0.742978\n",
      "4 -0.127257  0.652142 -0.948329\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0 -0.722593  0.608537 -0.909785\n",
      "1 -0.307464 -0.436524 -0.601029\n",
      "2  0.510171 -0.338077  0.224081\n",
      "3  0.049980  0.306230  0.126814\n",
      "4 -0.889181  0.538261 -0.553405\n",
      "Prediction for the random data: ='WALKING'\n",
      "       accx      accy      accz\n",
      "0 -0.308523  0.375486  0.633201\n",
      "1  0.507949 -0.374635  0.052467\n",
      "2  0.862335  0.453301 -0.285743\n",
      "3 -0.519897  0.520324  0.319997\n",
      "4  0.805536 -0.706473  0.001633\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0  0.148284 -0.636446  0.673779\n",
      "1  0.459367  0.715010 -0.779902\n",
      "2  0.436643 -0.187411 -0.885731\n",
      "3  0.180823 -0.839379  0.343762\n",
      "4  0.606678  0.714484 -0.008213\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0  0.218621  0.184639 -0.021026\n",
      "1  0.382638 -0.091631 -0.055060\n",
      "2 -0.543091 -0.197608  0.188010\n",
      "3 -0.545442  0.299030 -0.984168\n",
      "4  0.180162 -0.857444 -0.393621\n",
      "Prediction for the random data: ='WALKING_UPSTAIRS'\n",
      "       accx      accy      accz\n",
      "0 -0.803738  0.902307 -0.082061\n",
      "1  0.594994 -0.218938 -0.096130\n",
      "2  0.483094 -0.016742  0.969093\n",
      "3 -0.557641 -0.806719 -0.801226\n",
      "4  0.125700  0.396533  0.017753\n",
      "Prediction for the random data: ='WALKING'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "for _ in range(10):\n",
    "    # Generate random accelerometer data with the same dimensions and range as typical input\n",
    "    num_samples = 500\n",
    "    accx = np.random.uniform(-1, 1, num_samples)\n",
    "    accy = np.random.uniform(-1, 1, num_samples)\n",
    "    accz = np.random.uniform(-1, 1, num_samples)\n",
    "\n",
    "    # Create a DataFrame to represent the random data\n",
    "    random_data = pd.DataFrame({\n",
    "        'accx': accx,\n",
    "        'accy': accy,\n",
    "        'accz': accz\n",
    "    })\n",
    "\n",
    "    # Display the first few rows of the random data\n",
    "    print(random_data.head())\n",
    "\n",
    "    # Function to sample data\n",
    "    def sample_data(data, sample_rate=10):\n",
    "        return data[::sample_rate]\n",
    "\n",
    "    # Create a Few-Shot Learning prompt\n",
    "    query_few_shot_3 = f\"\"\"\n",
    "    * You are a highly accurate activity classification model.\n",
    "    * Your task is to classify human activities based on the given accelerometer data.\n",
    "    * The accelerometer data is provided as mean acceleration values in the x, y, and z directions.\n",
    "    * You are given data corresponding to six different activities.\n",
    "    * The possible activities to classify are: LAYING, STANDING, WALKING, SITTING, WALKING UPSTAIRS, and WALKING DOWNSTAIRS.\"\"\"\n",
    "\n",
    "    train_data_df = pd.read_csv('train_data_combined.csv')\n",
    "    train_subjects = [1, 3, 5, 7]\n",
    "    train_data_df = train_data_df[train_data_df['Subject'].isin(train_subjects)]\n",
    "\n",
    "    for (train_subject, train_activity), grp in train_data_df.groupby(['Subject', 'Activity']):\n",
    "        query_few_shot_3 += \"\"\"Here are some examples of accelerometer data and their corresponding activities:\"\"\"\n",
    "        query_few_shot_3 += f\"\"\"\n",
    "        * Activity: {train_activity}\n",
    "        accx = {sample_data(grp['accx'].tolist(), 50)}\n",
    "        accy = {sample_data(grp['accy'].tolist(), 50)}\n",
    "        accz = {sample_data(grp['accz'].tolist(), 50)}\n",
    "        \"\"\"\n",
    "\n",
    "    # Add the random accelerometer data to the prompt\n",
    "    query_few_shot_3 += f\"\"\"\n",
    "    * Analyze the accelerometer data and provide the most likely activity label for this case.\n",
    "    * PRINT ONLY A WORD WHICH IS THE PREDICTED ACTIVITY AND NOTHING ELSE NO CONTENT NO REASON JUST A PREDICTION\n",
    "\n",
    "    accx = {sample_data(random_data['accx'].tolist(), 10)}\n",
    "    accy = {sample_data(random_data['accy'].tolist(), 10)}\n",
    "    accz = {sample_data(random_data['accz'].tolist(), 10)}\n",
    "    \"\"\"\n",
    "\n",
    "    # Simulate model prediction (replace with actual model prediction code)\n",
    "    result = llm.invoke(query_few_shot_3)\n",
    "\n",
    "    print(\"Prediction for the random data:\", str(result).split(\" \")[0][7:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
